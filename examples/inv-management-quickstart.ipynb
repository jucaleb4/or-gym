{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "937597e4",
   "metadata": {},
   "source": [
    "# How to Use Deep Reinforcement Learning to Improve your Supply Chain\n",
    "\n",
    "Full write up available [here](https://www.datahubbs.com/how-to-use-deep-reinforcement-learning-to-improve-your-supply-chain/).\n",
    "\n",
    "Note Ray is not a dependency of OR-Gym. We want OR-Gym to be able to stand independently of other RL libraries as much as possible.\n",
    "\n",
    "There have been breaking changes that have been introduced in later version of Ray which affect this environment in particular. To ensure no conflicts, please run:\n",
    "- `pip install ray==1.0.0`\n",
    "- `pip install ray[rllib]`\n",
    "- `pip install ray[tune]`\n",
    "- `pip install tensorflow==2.3.0`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefefc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_gym\n",
    "from or_gym.utils import create_env\n",
    "\n",
    "# import ray\n",
    "# from ray.rllib.agents.ppo import PPOTrainer\n",
    "# from ray import tune\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Environment and RL Configuration Settings\n",
    "env_name = 'InvManagement-v1'\n",
    "# env_name = \"Knapsack-v0\"\n",
    "env_config = {} # Change environment parameters here\n",
    "\n",
    "env = Monitor(env=or_gym.make(env_name))\n",
    "# env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13304f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Ray and Build Agent\n",
    "\n",
    "callback_max_episodes = StopTrainingOnMaxEpisodes(\n",
    "    max_episodes=5000, \n",
    "    verbose=1\n",
    ")\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(np.inf, callback=callback_max_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb31e9-6647-4c10-914f-02a663c068b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rwd_arr = env.get_episode_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c258e590-3654-47e8-9d10-bb6aaee84d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(rwd_arr)\n",
    "ax.set(\n",
    "    xlabel=\"episodes\", \n",
    "    ylabel=\"reward\", \n",
    "    title=f\"PPO training on {env_name}\",\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack values from each iteration\n",
    "rewards = np.hstack([i['hist_stats']['episode_reward'] \n",
    "    for i in results])\n",
    "pol_loss = [\n",
    "    i['info']['learner']['default_policy']['policy_loss'] \n",
    "    for i in results]\n",
    "vf_loss = [\n",
    "    i['info']['learner']['default_policy']['vf_loss'] \n",
    "    for i in results]\n",
    "p = 100\n",
    "mean_rewards = np.array([np.mean(rewards[i-p:i+1]) \n",
    "                if i >= p else np.mean(rewards[:i+1]) \n",
    "                for i, _ in enumerate(rewards)])\n",
    "std_rewards = np.array([np.std(rewards[i-p:i+1])\n",
    "               if i >= p else np.std(rewards[:i+1])\n",
    "               for i, _ in enumerate(rewards)])\n",
    "fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "gs = fig.add_gridspec(2, 4)\n",
    "ax0 = fig.add_subplot(gs[:, :-2])\n",
    "ax0.fill_between(np.arange(len(mean_rewards)), \n",
    "                 mean_rewards - std_rewards, \n",
    "                 mean_rewards + std_rewards, \n",
    "                 label='Standard Deviation', alpha=0.3)\n",
    "ax0.plot(mean_rewards, label='Mean Rewards')\n",
    "ax0.set_ylabel('Rewards')\n",
    "ax0.set_xlabel('Episode')\n",
    "ax0.set_title('Training Rewards')\n",
    "ax0.legend()\n",
    "ax1 = fig.add_subplot(gs[0, 2:])\n",
    "ax1.plot(pol_loss)\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_title('Policy Loss')\n",
    "ax2 = fig.add_subplot(gs[1, 2:])\n",
    "ax2.plot(vf_loss)\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_title('Value Function Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e494fbe",
   "metadata": {},
   "source": [
    "# Derivative Free Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741bbd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_stock_policy(policy, env):\n",
    "  '''\n",
    "  Implements a re-order up-to policy. This means that for\n",
    "  each node in the network, if the inventory at that node \n",
    "  falls below the level denoted by the policy, we will \n",
    "  re-order inventory to bring it to the policy level.\n",
    "  \n",
    "  For example, policy at a node is 10, current inventory\n",
    "  is 5: the action is to order 5 units.\n",
    "  '''\n",
    "  assert len(policy) == len(env.init_inv), (\n",
    "    'Policy should match number of nodes in network' + \n",
    "    '({}, {}).'.format(len(policy), len(env.init_inv)))\n",
    "  \n",
    "  # Get echelon inventory levels\n",
    "  if env.period == 0:\n",
    "    inv_ech = np.cumsum(env.I[env.period] +\n",
    "      env.T[env.period])\n",
    "  else:\n",
    "    inv_ech = np.cumsum(env.I[env.period] +\n",
    "      env.T[env.period] - env.B[env.period-1, :-1])\n",
    "      \n",
    "  # Get unconstrained actions\n",
    "  unc_actions = policy - inv_ech\n",
    "  unc_actions = np.where(unc_actions>0, unc_actions, 0)\n",
    "  \n",
    "  # Ensure that actions can be fulfilled by checking \n",
    "  # constraints\n",
    "  inv_const = np.hstack([env.I[env.period, 1:], np.Inf])\n",
    "  actions = np.minimum(env.c, np.minimum(unc_actions, inv_const))\n",
    "  return actions\n",
    "\n",
    "def dfo_func(policy, env, *args):\n",
    "    '''\n",
    "    Runs an episode based on current base-stock model \n",
    "    settings. This allows us to use our environment for the \n",
    "    DFO optimizer.\n",
    "    '''\n",
    "    env.reset() # Ensure env is fresh\n",
    "    rewards = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = base_stock_policy(policy, env)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    rewards = np.array(rewards)\n",
    "    prob = env.demand_dist.pmf(env.D, **env.dist_param)\n",
    "    \n",
    "    # Return negative of expected profit\n",
    "    return -1 / env.num_periods * np.sum(prob * rewards)\n",
    "  \n",
    "def optimize_inventory_policy(env_name, fun,\n",
    "  init_policy=None, env_config={}, method='Powell'):\n",
    "  \n",
    "  env = or_gym.make(env_name, env_config=env_config)\n",
    "  \n",
    "  if init_policy is None:\n",
    "      init_policy = np.ones(env.num_stages-1)\n",
    "      \n",
    "  # Optimize policy\n",
    "  out = minimize(fun=fun, x0=init_policy, args=env, \n",
    "      method=method)\n",
    "  policy = out.x.copy()\n",
    "  \n",
    "  # Policy must be positive integer\n",
    "  policy = np.round(np.maximum(policy, 0), 0).astype(int)\n",
    "  \n",
    "  return policy, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, out = optimize_inventory_policy('InvManagement-v1',\n",
    "    dfo_func)\n",
    "print(\"Re-order levels: {}\".format(policy))\n",
    "print(\"DFO Info:\\n{}\".format(out))\n",
    "\n",
    "env = or_gym.make(env_name, env_config=env_config)\n",
    "eps = 1000\n",
    "rewards = []\n",
    "for i in range(eps):\n",
    "    env.reset()\n",
    "    reward = 0\n",
    "    while True:\n",
    "        action = base_stock_policy(policy, env)\n",
    "        s, r, done, _ = env.step(action)\n",
    "        reward += r\n",
    "        if done:\n",
    "            rewards.append(reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5147b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc8a2230aa8b659650bd48bf6a546b4d453aa64d7078ee0770a23a54a48157c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
